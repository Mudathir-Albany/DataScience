---
title: "shopper prediction markdown"
author: "Mudathir Salahudeen"
date: "2022-09-28"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
shop<-read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00468/online_shoppers_intention.csv")

table(shop$Revenue)
```

```{r}
table(shop$Revenue)

install.packages("funModeling")
install.packages("GGally") 
#extends ggplot2 by adding several functions to reduce the complexity
#of combining geoms with transformed data
install.packages("tidyverse")
install.packages("MLmetrics")
install.packages("caret")
install.packages("C50")
install.packages("class")
install.packages("e1071")
install.packages("SuperLearner")
install.packages("ranger")
install.packages("kernlab")
install.packages("ipred")
install.packages("arm")

```


```{r}
library(funModeling)
library(GGally)
library(tidyverse)
library(MLmetrics)
library(caret)
library(C50)
library(class)
library(e1071)
library(SuperLearner)
library(ranger)
library(kernlab)
library(ipred)
library(arm)
```

#EXPLORING THE DATA
```{r}
summary(shop)
str(shop)
nrow(shop)
```

#Frequency Plot for numerical data - using funmodelling package
```{r}
shop_num <- shop[1:10] 

print(plot_num(shop_num)) # Histogram of all the continuous variables
```

#Frequency Plot for Categorical data - using funmodelling package
```{r}
shop_cat <- shop[11:18]

#To work with factors, weâ€™ll use the forcats package, which is part of the core tidyverse.
#It provides tools for dealing with categorical variables

shop_cat$OperatingSystems<- as.factor(shop_cat$OperatingSystems)
shop_cat$Browser<- as.factor(shop_cat$Browser)
shop_cat$Region<- as.factor(shop_cat$Region)
shop_cat$TrafficType<- as.factor(shop_cat$TrafficType)
shop_cat$Weekend<- as.factor(shop$Weekend)
shop_cat$Revenue<- as.factor(shop_cat$Revenue)

print(freq(shop_cat)) # Frequency & frequency plots of catergorical variables

#NOTE:
#factors are used to work with categorical variables, variables that have a fixed and known set of possible values. They are also useful when you want to display character vectors in a non-alphabetical order.
```

#CORRELATION OF THE WHOLE SHOP DATA
```{r}
data<- shop

#They are categorical and needed to be factored to numeric.
#The as.numeric in R is a built-in method that returns a numeric value. The as.numeric() function takes an R object that needs to be coerced and returns the converted numeric value.

data$Month<- as.numeric(data$Month)
data$VisitorType<- as.numeric(data$VisitorType)

M<-cor(data)
ggcorr(M)
```
#Average Time Spent on Pages
```{r}
Avg_Time <- as.matrix(data.frame(data %>%
  summarise(Administration= mean(Administrative_Duration),
            information= mean(Informational_Duration),
            Product=mean(ProductRelated_Duration)
            )))
barplot(Avg_Time, main = "Average Time Spent on Pages", ylab="Time", xlab = "Pages", col=c("#57CC99"))
```
#Region with Highest Customers
```{r}
attach(shop)
Reg_access <- as.data.frame(shop %>% 
  group_by(Region) %>%
  count())

Reg_access
barplot(height=Reg_access$n, main = "Region with Highest Customers", ylab="Count", xlab = "Region", col=c("#57CC99"), names.arg = c("Reg 1","Reg 2","Reg 3","Reg 4","Reg 5","Reg 6","Reg 7","Reg 8", "Reg 9"))
```


#PLOT OF BOUNCE TIME AGAINST REVENUE
```{r}
plot(shop$BounceRates,shop$Revenue,  main= "Relationship between Revenue and Bounce Rate",
      xlab = "BounceRates",ylab="Revenue")
# We can notice when the bounce rate is close to zero,The customer is more likely to develop revenue.
#Bounce rate is an Internet marketing term used in web traffic analysis. It represents the percentage of visitors who enter the
#site and then leave ("bounce") rather than continuing to view other pages within the same site.
```

#PLOT OF EXIT TIME AGAINST REVENUE
```{r}
plot(shop$ExitRates,shop$Revenue,xlab = "ExitRates",ylab = "Revenue",main= "Relationship between Revenue and Exit Rate")
```
```{r}
data %>%
  filter(Revenue) %>%
  ggplot(aes(x=BounceRates, y= ExitRates))+
  geom_point(alpha=0.5,color = "darkblue") +
  geom_smooth(se = 0) +
  ggtitle("Relationship between Bounce Rate and Exit Rate")
```

#WHAT MONTH HAS THE HIGHEST SALE?
```{r}
data1<- shop %>%
  filter(shop$Revenue==1)


ggplot(data1, aes(Month, ..count..)) + geom_bar(aes(fill = Revenue), position = "dodge") + labs(x="Month",y="Revenue")
# We can notice most number of transactions occur in the month of November
```

#WHAT TYPE OF VISITORS GENERATED THE MOST REVENUE(NEW? RETURNING? OR OTHER?)
```{r}
shop %>%
  filter(Revenue == 1) %>%
  
  ggplot(aes(x=Month, fill = VisitorType ))+
  geom_bar(alpha=0.8,color = "black") + ggtitle("Monthly Revenue by types of Visitors")
```

#WHICH DAYS OF THE WEEK GENERATED MORE REVENUE (WEEKENDS? OR WEEKDAYS?)
```{r}
shop %>%
  filter(Revenue) %>%
  ggplot(aes(x=Month, fill = Weekend ))+
  geom_bar(alpha=0.8,color = "black") + ggtitle("Monthly Revenue (Weekdays and Weekends)")
```

PREDICTIVE ANALYSIS

#Fitting a Decison Tree
```{r}
shop1=shop

shop1$Revenue<- as.factor(shop1$Revenue)
shop1$Weekend<- as.factor(shop1$Weekend)

index <- createDataPartition(shop1$Revenue, p=0.75, list=FALSE)
train <-shop1[ index,]
test <- shop1[-index,]

install.packages("RWeka")
library(RWeka)
fit<-J48(Revenue ~.,data=train)
summary(fit)
p_tree<-predict(fit,test[,1:17])
confusionMatrix(p_tree,test$Revenue)

# C5.0 Boosted trees

dtree<-C5.0(train,train$Revenue)
plot(dtree)

p_dtree<-predict(dtree,test)

confusionMatrix(table(p_dtree,test$Revenue))
Accuracy(p_dtree,test$Revenue)

# C5.0 Decision tree produces 100% accuracy
```

#Naive Bayes
```{r}
x=train #train & test can be found in the decison tree code block
y=train$Revenue

model = naiveBayes(x,y)
p<- predict(model,test,type="class")

confusionMatrix(p,test$Revenue)
```

#Ensemble
```{r}
set.seed(667)
shop3=shop

shop3$Revenue<- as.numeric(shop3$Revenue)
shop3$Weekend<-as.factor(shop3$Weekend)

index3 <- createDataPartition(shop3$Revenue, p=0.75, list=FALSE)
train3 <-shop3[ index3,]
test3 <- shop3[-index3,]

xtrain <- data.frame(train3[,1:17])
xtest <- data.frame(test3[,1:17])

y_lab=(train3[,18])
ytest_lab=as.numeric((test3[,18]))


                    #### With Kernel Support Vector Machines, Bayes GLM and Bagging ###

# Fit the ensemble model
model <- SuperLearner(y_lab,xtrain,family=binomial(),SL.library=list("SL.ranger","SL.ipredbagg","SL.bayesglm"))

# BayesGLM for logistic regression,Ranger for random Forest, ipredbagg for Bagging
model

# We can see the Risk associated with all the three models are quite low
```

#Ensemble prediction
```{r}
predictions <- predict.SuperLearner(model, newdata=as.data.frame(xtest))

head(predictions$library.predict) # Predictions of Individual libraries

 #Recoding the Probabilities
conv.preds <- ifelse(predictions$pred>=0.5,1,0)

cm <- Accuracy(conv.preds,ytest_lab)
round(cm,2)
```



#KNN CLASSIFICATION
```{r}
shop2=shop

shop2$Month=as.numeric(shop1$Month)
shop2$VisitorType=as.numeric(shop1$VisitorType)
shop2$Weekend=as.numeric(shop1$Weekend)
shop2$Revenue=as.numeric(shop1$Revenue) 

#All the above variables are converted to numeric as KNN takes only numeric inputs

index1 <- createDataPartition(shop1$Revenue, p=0.75, list=FALSE)
train1 <-shop2[ index1,]
test1 <- shop2[-index1,]

train1_lab<-train1$Revenue
test1_lab<-test1$Revenue

normalize <- function(x) {
 return ((x - min(x)) / (max(x) - min(x)))
 }
  
shop2[1:6]<-lapply(shop2[1:6],normalize)

library(class)
knn_pred<-knn(train1,test1,train1_lab,k=111)

confusionMatrix(table(knn_pred,test1$Revenue))
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```

